{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "9d2dbdb3-6c74-4f96-9865-2951dfd653ce",
        "_uuid": "bb41ad86b25fecf332927b0c8f55dd710101e33f"
      },
      "cell_type": "markdown",
      "source": "# Improved LSTM baseline\n\nThis kernel is a somewhat improved version of [Keras - Bidirectional LSTM baseline](https://www.kaggle.com/CVxTz/keras-bidirectional-lstm-baseline-lb-0-051) along with some additional documentation of the steps. (NB: this notebook has been re-run on the new test set.)"
    },
    {
      "metadata": {
        "_cell_guid": "2f9b7a76-8625-443d-811f-8f49781aef81",
        "_uuid": "598f965bc881cfe6605d92903b758778d400fa8b",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import sys, os, re, csv, codecs, numpy as np, pandas as pd\nfrom sklearn.metrics import f1_score, make_scorer, classification_report\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Using TensorFlow backend.\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7fd4f0d798c5542ee8f707ea68e85008f6041d74",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "df = pd.read_csv('../input/disaster-data/disaster_data.csv')",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  interactivity=interactivity, compiler=compiler, result=result)\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2b741c1375113fd7cc281a6214440aa506408b23"
      },
      "cell_type": "code",
      "source": "X = df['message']\nY = df.drop(['Unnamed: 0', 'id', 'message', 'genre', 'original', 'child_alone'], axis=1)",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "c297fa80-beea-464b-ac90-f380ebdb02fe",
        "_uuid": "d961885dfde18796893922f72ade1bf64456404e"
      },
      "cell_type": "markdown",
      "source": "We include the GloVe word vectors in our input files. To include these in your kernel, simple click 'input files' at the top of the notebook, and search 'glove' in the 'datasets' section."
    },
    {
      "metadata": {
        "_cell_guid": "66a6b5fd-93f0-4f95-ad62-3253815059ba",
        "_uuid": "729b0f0c2a02c678631b8c072d62ff46146a82ef",
        "trusted": true
      },
      "cell_type": "code",
      "source": "path = '../input/'\ncomp = 'jigsaw-toxic-comment-classification-challenge/'\nEMBEDDING_FILE=f'{path}glove6b50d/glove.6B.50d.txt'\nTRAIN_DATA_FILE=f'{path}{comp}train.csv'\nTEST_DATA_FILE=f'{path}{comp}test.csv'",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "98f2b724-7d97-4da8-8b22-52164463a942",
        "_uuid": "b62d39216c8d00b3e6b78b825212fd190757dff9"
      },
      "cell_type": "markdown",
      "source": "Set some basic config parameters:"
    },
    {
      "metadata": {
        "_cell_guid": "2807a0a5-2220-4af6-92d6-4a7100307de2",
        "_uuid": "d365d5f8d9292bb9bf57d21d6186f8b619cbe8c3",
        "trusted": true
      },
      "cell_type": "code",
      "source": "embed_size = 50 # how big is each word vector\nmax_features = 20000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 200 # max number of words in a comment to use",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "b3a8d783-95c2-4819-9897-1320e3295183",
        "_uuid": "4dd8a02e7ef983f10ec9315721c6dda2958024af"
      },
      "cell_type": "markdown",
      "source": "Read in our data and replace missing values:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "183fce594106121f6c3a40c9c0fba818bd1501d3"
      },
      "cell_type": "code",
      "source": "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "ac2e165b-1f6e-4e69-8acf-5ad7674fafc3",
        "_uuid": "8ab6dad952c65e9afcf16e43c4043179ef288780",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# train = pd.read_csv(TRAIN_DATA_FILE)\n# test = pd.read_csv(TEST_DATA_FILE)\n\nlist_sentences_train = X_train.values\nlist_classes = Y.columns\ny = y_train\nlist_sentences_test = X_test.values",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "54a7a34e-6549-45f7-ada2-2173ff2ce5ea",
        "_uuid": "e8810c303980f41dbe0543e1c15d35acbdd8428f"
      },
      "cell_type": "markdown",
      "source": "Standard keras preprocessing, to turn each comment into a list of word indexes of equal length (with truncation or padding as needed)."
    },
    {
      "metadata": {
        "_cell_guid": "79afc0e9-b5f0-42a2-9257-a72458e91dbb",
        "_uuid": "c292c2830522bfe59d281ecac19f3a9415c07155",
        "trusted": true
      },
      "cell_type": "code",
      "source": "tokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(list_sentences_train))\nlist_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\nlist_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\nX_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\nX_te = pad_sequences(list_tokenized_test, maxlen=maxlen)",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "f8c4f6a3-3a19-40b1-ad31-6df2690bec8a",
        "_uuid": "e1cb77629e35c2b5b28288b4d6048a86dda04d78"
      },
      "cell_type": "markdown",
      "source": "Read the glove word vectors (space delimited strings) into a dictionary from word->vector."
    },
    {
      "metadata": {
        "_cell_guid": "7d19392b-7750-4a1b-ac30-ed75b8a62d52",
        "_uuid": "e9e3b4fa7c4658e0f22dd48cb1a289d9deb745fc",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE))",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "7370416a-094a-4dc7-84fa-bdbf469f6579",
        "_uuid": "20cea54904ac1eece20874e9346905a59a604985"
      },
      "cell_type": "markdown",
      "source": "Use these vectors to create our embedding matrix, with random initialization for words that aren't in GloVe. We'll use the same mean and stdev of embeddings the GloVe has when generating the random init."
    },
    {
      "metadata": {
        "_cell_guid": "4d29d827-377d-4d2f-8582-4a92f9569719",
        "_uuid": "96fc33012e7f07a2169a150c61574858d49a561b",
        "trusted": true
      },
      "cell_type": "code",
      "source": "all_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nemb_mean,emb_std",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n  \"\"\"Entry point for launching an IPython kernel.\n",
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "(0.020940498, 0.6441043)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "62acac54-0495-4a26-ab63-2520d05b3e19",
        "_uuid": "574c91e270add444a7bc8175440274bdd83b7173",
        "trusted": true
      },
      "cell_type": "code",
      "source": "word_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector",
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "f1aeec65-356e-4430-b99d-bb516ec90b09",
        "_uuid": "237345510bd2e664b5c6983a698d80bac2732bc4"
      },
      "cell_type": "markdown",
      "source": "Simple bidirectional LSTM with two fully connected layers. We add some dropout to the LSTM since even 2 epochs is enough to overfit."
    },
    {
      "metadata": {
        "_cell_guid": "0d4cb718-7f9a-4eab-acda-8f55b4712439",
        "_uuid": "dc51af0bd046e1eccc29111a8e2d77bdf7c60d28",
        "trusted": true
      },
      "cell_type": "code",
      "source": "inp = Input(shape=(maxlen,))\nx = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\nx = Bidirectional(LSTM(100, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\nx = GlobalMaxPool1D()(x)\nx = Dense(64, activation=\"relu\")(x)\nx = Dropout(0.2)(x)\nx = Dense(64, activation=\"relu\")(x)\nx = Dropout(0.2)(x)\nx = Dense(len(Y.columns), activation=\"sigmoid\")(x)\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "4a624b55-3720-42bc-ad5a-7cefc76d83f6",
        "_uuid": "e2a0e9ce12e1ff5ea102665e79de23df5caf5802"
      },
      "cell_type": "markdown",
      "source": "Now we're ready to fit out model! Use `validation_split` when not submitting."
    },
    {
      "metadata": {
        "_cell_guid": "333626f1-a838-4fea-af99-0c78f1ef5f5c",
        "scrolled": false,
        "_uuid": "c1558c6b2802fc632edc4510c074555a590efbd8",
        "trusted": true
      },
      "cell_type": "code",
      "source": "hist = model.fit(X_t, y, batch_size=32, epochs=5, validation_split=0.2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "d6fa2ace-aa92-40cf-913f-a8f5d5a4b130",
        "_uuid": "3dbaa4d0c22271b8b0dc7e58bcad89ddc607beaf"
      },
      "cell_type": "markdown",
      "source": "And finally, get predictions for the test set and prepare a submission CSV:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2893e59b260d77f018d3ab0e9b48a5be6e4a341b"
      },
      "cell_type": "code",
      "source": "from keras.models import Sequential,Model\nfrom keras.layers import CuDNNLSTM, Dense, Bidirectional, Input,Dropout\n\nfrom keras import backend as K\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "42d0608a02f73da820535d9c8e4316f91faa6057"
      },
      "cell_type": "code",
      "source": "# https://www.kaggle.com/qqgeogor/keras-lstm-attention-glove840b-lb-0-043\nclass Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim",
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "47792257b97f3b3246b0f7c2ad07e192f30b98b8"
      },
      "cell_type": "code",
      "source": "inp = Input(shape=(maxlen,))\nx = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\nx = Bidirectional(LSTM(128, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n#x = Bidirectional(LSTM(64, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\nx = Attention(maxlen)(x)\n# x = GlobalMaxPool1D()(x)\nx = Dense(64, activation=\"relu\")(x)\nx = Dropout(0.2)(x)\n# x = Dense(64, activation=\"relu\")(x)\n# x = Dropout(0.2)(x)\nx = Dense(len(Y.columns), activation=\"sigmoid\")(x)\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])",
      "execution_count": 16,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "63e0af8e3af9bc541f811fef65f0830cf687e0a8"
      },
      "cell_type": "code",
      "source": "hist = model.fit(X_t, y, batch_size=32, epochs=5, validation_split=0.2)",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Train on 16777 samples, validate on 4195 samples\nEpoch 1/5\n16777/16777 [==============================] - 230s 14ms/step - loss: 0.2370 - acc: 0.9180 - val_loss: 0.1868 - val_acc: 0.9353\nEpoch 2/5\n16777/16777 [==============================] - 225s 13ms/step - loss: 0.1838 - acc: 0.9365 - val_loss: 0.1672 - val_acc: 0.9432\nEpoch 3/5\n16777/16777 [==============================] - 223s 13ms/step - loss: 0.1683 - acc: 0.9420 - val_loss: 0.1596 - val_acc: 0.9458\nEpoch 4/5\n16777/16777 [==============================] - 227s 14ms/step - loss: 0.1595 - acc: 0.9446 - val_loss: 0.1560 - val_acc: 0.9471\nEpoch 5/5\n16777/16777 [==============================] - 229s 14ms/step - loss: 0.1524 - acc: 0.9466 - val_loss: 0.1573 - val_acc: 0.9468\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e1d6f74c1adf42490fa48d89c663cc59ef540677"
      },
      "cell_type": "code",
      "source": "y_preds = model.predict([X_te], batch_size=1024, verbose=1)\npreds = (y_preds > 0.5).astype(np.int)\nprint(classification_report(y_test.values, preds, \n                            target_names = y_test.columns))",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": "5244/5244 [==============================] - 12s 2ms/step\n                        precision    recall  f1-score   support\n\n               related       0.89      0.90      0.89      3978\n               request       0.85      0.57      0.68       895\n                 offer       0.00      0.00      0.00        26\n           aid_related       0.79      0.66      0.72      2131\n          medical_help       0.66      0.05      0.08       422\n      medical_products       0.89      0.03      0.06       270\n     search_and_rescue       0.00      0.00      0.00       127\n              security       0.00      0.00      0.00        88\n              military       1.00      0.01      0.01       155\n                 water       0.80      0.18      0.29       339\n                  food       0.76      0.43      0.55       595\n               shelter       0.86      0.05      0.10       470\n              clothing       0.00      0.00      0.00        73\n                 money       0.00      0.00      0.00       104\n        missing_people       0.00      0.00      0.00        60\n              refugees       0.00      0.00      0.00       171\n                 death       1.00      0.01      0.02       237\n             other_aid       0.00      0.00      0.00       695\ninfrastructure_related       0.50      0.00      0.01       328\n             transport       0.00      0.00      0.00       240\n             buildings       0.67      0.01      0.03       267\n           electricity       0.00      0.00      0.00       122\n                 tools       0.00      0.00      0.00        32\n             hospitals       0.00      0.00      0.00        46\n                 shops       0.00      0.00      0.00        22\n           aid_centers       0.00      0.00      0.00        67\n  other_infrastructure       0.00      0.00      0.00       223\n       weather_related       0.82      0.78      0.80      1438\n                floods       0.70      0.36      0.48       411\n                 storm       0.60      0.57      0.58       486\n                  fire       0.00      0.00      0.00        53\n            earthquake       0.88      0.78      0.83       478\n                  cold       0.00      0.00      0.00       117\n         other_weather       0.00      0.00      0.00       276\n         direct_report       0.76      0.49      0.60      1021\n\n             micro avg       0.83      0.50      0.63     16463\n             macro avg       0.38      0.17      0.19     16463\n          weighted avg       0.69      0.50      0.54     16463\n           samples avg       0.62      0.43      0.47     16463\n\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n  'precision', 'predicted', average, warn_for)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n  'precision', 'predicted', average, warn_for)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n  'recall', 'true', average, warn_for)\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "80ca0031caaddc1c268b3eed409bfc52ad3f4d31"
      },
      "cell_type": "markdown",
      "source": "# Build Naive Bayes - SVM models\n\nPaper: Baselines and Bigrams: Simple, Good Sentiment and Topic Classification, Sida Wang and Christopher D. Manning (https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf)\n\nReference: AlexSÃ¡nchez's comment on https://www.kaggle.com/jhoward/nb-svm-strong-linear-baseline"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "59ccd7047c7d34de6da82207edd578187d955912"
      },
      "cell_type": "code",
      "source": "from sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils.validation import check_X_y, check_is_fitted\nfrom sklearn.linear_model import LogisticRegression\nfrom scipy import sparse\nimport numpy as np\n\nclass NbSvmClassifier(BaseEstimator, ClassifierMixin):\n    def __init__(self, C=1.0, dual=False, n_jobs=1):\n        self.C = C\n        self.dual = dual\n        self.n_jobs = n_jobs\n\n    def predict(self, x):\n        # Verify that model has been fit\n        check_is_fitted(self, ['_r', '_clf'])\n        return self._clf.predict(x.multiply(self._r))\n\n    def predict_proba(self, x):\n        # Verify that model has been fit\n        check_is_fitted(self, ['_r', '_clf'])\n        return self._clf.predict_proba(x.multiply(self._r))\n\n    def fit(self, x, y):\n        # Check that X and y have correct shape\n        #y = y.values\n        x, y = check_X_y(x, y, accept_sparse=True)\n\n        def pr(x, y_i, y):\n            p = x[y==y_i].sum(0)\n            return (p+1) / ((y==y_i).sum()+1)\n\n        self._r = sparse.csr_matrix(np.log(pr(x,1,y) / pr(x,0,y)))\n        x_nb = x.multiply(self._r)\n        self._clf = LogisticRegression(C=self.C, dual=self.dual, n_jobs=self.n_jobs).fit(x_nb, y)\n        return self",
      "execution_count": 19,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f4f9d9974b043e9685ab6c1d186fa08f50436434"
      },
      "cell_type": "code",
      "source": "# Text Processing\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\n\n# Modeling\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.metrics import f1_score, make_scorer, classification_report\n\n\ndef load_data(database_filepath):\n    '''\n    Load data from sqlite database, quick cleaning, and\n    return the features and labels of the models.\n\n    INPUTS:\n        database_filepath: path to the sqlite database\n\n    OUTPUTS:\n        X, Y: features and labels of the model\n        Y.columns: categories of the label\n    '''\n\n    engine = create_engine('sqlite:///' + database_filepath)\n    df = pd.read_sql_table('DisasterResponse', engine)\n\n    # 'related' column has values of 0,1,2 which doesn't make sense for binary classification\n    df['related'] = df['related'].replace(2, 1)\n\n    # 'child_alone' column has only value of 0. So our model will always predict 0.\n    df = df.drop('child_alone', axis=1)\n\n    X = df['message']\n    Y = df.drop(['id', 'message', 'genre', 'original'], axis=1)\n\n    return X, Y, Y.columns\n\n\ndef tokenize(text):\n    '''\n    Preprocess text features by tokenization and lemmatization.\n\n    INPUTS:\n        text: string of text need to be processed\n\n    OUTPUTS:\n        tokens: a list of tokens from the text\n    '''\n\n    # normalize case and remove punctuation\n    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n\n    # tokenize text\n    tokens = word_tokenize(text)\n\n    # lemmatize andremove stop words\n    lemmatizer = WordNetLemmatizer()\n    tokens = [lemmatizer.lemmatize(word) for word in tokens \\ \n              if word not in stopwords.words('english')]\n\n    return tokens\n\n\ndef scorer(y_test, y_pred):\n    '''\n    Create a evaluation metric for the grid search.\n    '''\n\n    report = classification_report(y_test, y_pred, output_dict=True)\n    weighted_avg = report['weighted avg']\n    return weighted_avg['f1-score']\n\n\ndef build_model(pretrained_model=None):\n    '''\n    Build ML pipeline to including text processing and multi-output multi-class classifier\n    If pretrained_model not None, load pretrained model. \n    Otherwise output model is a grid search, which takes longer to train.\n    '''\n\n    if pretrained_model != None:\n        clf = pickle.load(open(pretrained_model, 'rb'))\n    else:\n        pipeline = Pipeline([\n            ('countvec', CountVectorizer(tokenizer=tokenize)),\n            ('tfidf', TfidfTransformer()),\n            ('clf', MultiOutputClassifier(NbSvmClassifier()))\n        ])\n\n        parameters = {\n            'clf__estimator__C': [1.0, 5.0, 10.0],\n            'countvec__ngram_range': [(1, 1), (1, 2)]\n        }\n\n        f1_scorer = make_scorer(scorer)\n\n        # optimize model\n        clf = GridSearchCV(pipeline, parameters, scoring=f1_scorer,\n                           cv=3, verbose=10)\n\n    return clf\n\n\ndef evaluate_model(model, X_test, Y_test, category_names, pretrained_model=None):\n    '''\n    Evaluate the classifier by classification report (sklearn).\n    If the pretrained_model is None, we trained the grid search, \n    so best parameters will be reported.\n    '''\n    if pretrained_model == None:\n        print(\"Best parameters: \", model.best_params_)\n\n    Y_pred = model.predict(X_test)\n    report = classification_report(Y_test, Y_pred, target_names=category_names, \n                                   output_dict=True)\n\n    print(\"Validation Results: \")\n    print(classification_report(Y_test, Y_pred, target_names=category_names))\n\n    return (Y_pred, report)\n\n\ndef save_model(model, model_filepath):\n    '''\n    Save the model to a path specified by model_filepath.\n    '''\n    pickle.dump(model, open(model_filepath, 'wb'))\n    ",
      "execution_count": 27,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d905a6b75febf4b76fd71fe4ac00005464ea1e57"
      },
      "cell_type": "code",
      "source": "print('Building model...')\nmodel2 = build_model()\n\nprint('Training model...')\nmodel2.fit(X_train, y_train)\n\nprint('Evaluating model...')\npreds2, report = evaluate_model(model2, X_test, y_test, y_test.columns)\n\n# print('Saving model...\\n    MODEL: {}'.format(model_filepath))\n# save_model(model, model_filepath)\n",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Building model...\nTraining model...\nFitting 3 folds for each of 6 candidates, totalling 18 fits\n[CV] clf__estimator__C=1.0, countvec__ngram_range=(1, 1) .............\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n  'precision', 'predicted', average, warn_for)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n  'precision', 'predicted', average, warn_for)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n  'recall', 'true', average, warn_for)\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "[CV]  clf__estimator__C=1.0, countvec__ngram_range=(1, 1), score=0.6174687613469391, total= 1.1min\n[CV] clf__estimator__C=1.0, countvec__ngram_range=(1, 1) .............\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.8min remaining:    0.0s\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n  'precision', 'predicted', average, warn_for)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n  'precision', 'predicted', average, warn_for)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n  'recall', 'true', average, warn_for)\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "[CV]  clf__estimator__C=1.0, countvec__ngram_range=(1, 1), score=0.6257062108818834, total= 1.1min\n[CV] clf__estimator__C=1.0, countvec__ngram_range=(1, 1) .............\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  3.7min remaining:    0.0s\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n  'precision', 'predicted', average, warn_for)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n  'precision', 'predicted', average, warn_for)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n  'recall', 'true', average, warn_for)\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "[CV]  clf__estimator__C=1.0, countvec__ngram_range=(1, 1), score=0.6289740579661346, total= 1.1min\n[CV] clf__estimator__C=1.0, countvec__ngram_range=(1, 2) .............\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  5.5min remaining:    0.0s\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n  'precision', 'predicted', average, warn_for)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n  'precision', 'predicted', average, warn_for)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n  'recall', 'true', average, warn_for)\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "[CV]  clf__estimator__C=1.0, countvec__ngram_range=(1, 2), score=0.6241910613339897, total= 1.3min\n[CV] clf__estimator__C=1.0, countvec__ngram_range=(1, 2) .............\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  7.5min remaining:    0.0s\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n  'precision', 'predicted', average, warn_for)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n  'precision', 'predicted', average, warn_for)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n  'recall', 'true', average, warn_for)\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "[CV]  clf__estimator__C=1.0, countvec__ngram_range=(1, 2), score=0.6307677048001096, total= 1.3min\n[CV] clf__estimator__C=1.0, countvec__ngram_range=(1, 2) .............\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  9.6min remaining:    0.0s\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n  'precision', 'predicted', average, warn_for)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n  'precision', 'predicted', average, warn_for)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n  'recall', 'true', average, warn_for)\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "[CV]  clf__estimator__C=1.0, countvec__ngram_range=(1, 2), score=0.6329363273998921, total= 1.3min\n[CV] clf__estimator__C=5.0, countvec__ngram_range=(1, 1) .............\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed: 11.6min remaining:    0.0s\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n  'precision', 'predicted', average, warn_for)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n  'precision', 'predicted', average, warn_for)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n  'recall', 'true', average, warn_for)\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "[CV]  clf__estimator__C=5.0, countvec__ngram_range=(1, 1), score=0.6284620805508135, total= 1.1min\n[CV] clf__estimator__C=5.0, countvec__ngram_range=(1, 1) .............\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed: 13.4min remaining:    0.0s\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n  'precision', 'predicted', average, warn_for)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n  'precision', 'predicted', average, warn_for)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n  'recall', 'true', average, warn_for)\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "[CV]  clf__estimator__C=5.0, countvec__ngram_range=(1, 1), score=0.6379854020161618, total= 1.2min\n[CV] clf__estimator__C=5.0, countvec__ngram_range=(1, 1) .............\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed: 15.3min remaining:    0.0s\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n  'precision', 'predicted', average, warn_for)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n  'precision', 'predicted', average, warn_for)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n  'recall', 'true', average, warn_for)\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "[CV]  clf__estimator__C=5.0, countvec__ngram_range=(1, 1), score=0.6371521421092954, total= 1.1min\n[CV] clf__estimator__C=5.0, countvec__ngram_range=(1, 2) .............\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed: 17.0min remaining:    0.0s\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n  'precision', 'predicted', average, warn_for)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n  'precision', 'predicted', average, warn_for)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n  'recall', 'true', average, warn_for)\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "[CV]  clf__estimator__C=5.0, countvec__ngram_range=(1, 2), score=0.6414443281483035, total= 1.3min\n[CV] clf__estimator__C=5.0, countvec__ngram_range=(1, 2) .............\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n  'precision', 'predicted', average, warn_for)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n  'precision', 'predicted', average, warn_for)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n  'recall', 'true', average, warn_for)\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "[CV]  clf__estimator__C=5.0, countvec__ngram_range=(1, 2), score=0.6469481155431975, total= 1.4min\n[CV] clf__estimator__C=5.0, countvec__ngram_range=(1, 2) .............\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n  'precision', 'predicted', average, warn_for)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n  'precision', 'predicted', average, warn_for)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n  'recall', 'true', average, warn_for)\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "[CV]  clf__estimator__C=5.0, countvec__ngram_range=(1, 2), score=0.6507796391990328, total= 1.4min\n[CV] clf__estimator__C=10.0, countvec__ngram_range=(1, 1) ............\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n  'precision', 'predicted', average, warn_for)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n  'recall', 'true', average, warn_for)\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "[CV]  clf__estimator__C=10.0, countvec__ngram_range=(1, 1), score=0.6270712033821747, total= 1.1min\n[CV] clf__estimator__C=10.0, countvec__ngram_range=(1, 1) ............\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n  'precision', 'predicted', average, warn_for)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n  'precision', 'predicted', average, warn_for)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n  'recall', 'true', average, warn_for)\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "[CV]  clf__estimator__C=10.0, countvec__ngram_range=(1, 1), score=0.6349160837109199, total= 1.1min\n[CV] clf__estimator__C=10.0, countvec__ngram_range=(1, 1) ............\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n  'precision', 'predicted', average, warn_for)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n  'precision', 'predicted', average, warn_for)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n  'recall', 'true', average, warn_for)\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "[CV]  clf__estimator__C=10.0, countvec__ngram_range=(1, 1), score=0.6339367125638458, total= 1.1min\n[CV] clf__estimator__C=10.0, countvec__ngram_range=(1, 2) ............\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n  'precision', 'predicted', average, warn_for)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n  'precision', 'predicted', average, warn_for)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n  'recall', 'true', average, warn_for)\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "[CV]  clf__estimator__C=10.0, countvec__ngram_range=(1, 2), score=0.6409498568105442, total= 1.4min\n[CV] clf__estimator__C=10.0, countvec__ngram_range=(1, 2) ............\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n  'precision', 'predicted', average, warn_for)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n  'precision', 'predicted', average, warn_for)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n  'recall', 'true', average, warn_for)\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "[CV]  clf__estimator__C=10.0, countvec__ngram_range=(1, 2), score=0.648802592557683, total= 1.4min\n[CV] clf__estimator__C=10.0, countvec__ngram_range=(1, 2) ............\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n  'precision', 'predicted', average, warn_for)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n  'precision', 'predicted', average, warn_for)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n  'recall', 'true', average, warn_for)\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "[CV]  clf__estimator__C=10.0, countvec__ngram_range=(1, 2), score=0.6508290228506336, total= 1.4min\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "[Parallel(n_jobs=1)]: Done  18 out of  18 | elapsed: 35.1min finished\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "Evaluating model...\nBest parameters:  {'clf__estimator__C': 10.0, 'countvec__ngram_range': (1, 2)}\nValidation Results: \n                        precision    recall  f1-score   support\n\n               related       0.83      0.95      0.88      3978\n               request       0.80      0.58      0.67       895\n                 offer       0.00      0.00      0.00        26\n           aid_related       0.74      0.72      0.73      2131\n          medical_help       0.61      0.32      0.42       422\n      medical_products       0.70      0.32      0.44       270\n     search_and_rescue       0.71      0.17      0.28       127\n              security       0.40      0.02      0.04        88\n              military       0.54      0.34      0.42       155\n                 water       0.73      0.63      0.68       339\n                  food       0.83      0.72      0.77       595\n               shelter       0.77      0.59      0.67       470\n              clothing       0.78      0.44      0.56        73\n                 money       0.51      0.24      0.33       104\n        missing_people       1.00      0.15      0.26        60\n              refugees       0.63      0.30      0.41       171\n                 death       0.80      0.49      0.60       237\n             other_aid       0.51      0.14      0.22       695\ninfrastructure_related       0.42      0.06      0.11       328\n             transport       0.63      0.22      0.32       240\n             buildings       0.75      0.39      0.51       267\n           electricity       0.63      0.27      0.38       122\n                 tools       0.00      0.00      0.00        32\n             hospitals       0.25      0.04      0.07        46\n                 shops       0.00      0.00      0.00        22\n           aid_centers       0.50      0.01      0.03        67\n  other_infrastructure       0.22      0.02      0.03       223\n       weather_related       0.82      0.76      0.79      1438\n                floods       0.86      0.59      0.70       411\n                 storm       0.70      0.64      0.67       486\n                  fire       0.65      0.25      0.36        53\n            earthquake       0.86      0.72      0.78       478\n                  cold       0.69      0.34      0.46       117\n         other_weather       0.57      0.17      0.26       276\n         direct_report       0.70      0.46      0.55      1021\n\n             micro avg       0.78      0.62      0.69     16463\n             macro avg       0.60      0.34      0.41     16463\n          weighted avg       0.74      0.62      0.65     16463\n           samples avg       0.63      0.53      0.53     16463\n\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n  'precision', 'predicted', average, warn_for)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n  'precision', 'predicted', average, warn_for)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n  'recall', 'true', average, warn_for)\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "4988978e91827bea5e2a7709de04768049feb610"
      },
      "cell_type": "markdown",
      "source": "# Simple Ensemble"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "45123e35ed91a3f8e7a397cdf39b8e93e8de9ca3"
      },
      "cell_type": "code",
      "source": "ens_preds = (preds + preds2) / 2\nens_preds = (ens_preds >= 0.5).astype(np.int)\nprint(classification_report(y_test, ens_preds, target_names=y_test.columns))",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": "                        precision    recall  f1-score   support\n\n               related       0.82      0.97      0.89      3978\n               request       0.78      0.65      0.71       895\n                 offer       0.00      0.00      0.00        26\n           aid_related       0.73      0.77      0.75      2131\n          medical_help       0.60      0.33      0.42       422\n      medical_products       0.70      0.33      0.45       270\n     search_and_rescue       0.71      0.17      0.28       127\n              security       0.40      0.02      0.04        88\n              military       0.54      0.34      0.42       155\n                 water       0.70      0.64      0.67       339\n                  food       0.76      0.75      0.75       595\n               shelter       0.77      0.60      0.67       470\n              clothing       0.78      0.44      0.56        73\n                 money       0.51      0.24      0.33       104\n        missing_people       1.00      0.15      0.26        60\n              refugees       0.63      0.30      0.41       171\n                 death       0.80      0.49      0.61       237\n             other_aid       0.51      0.14      0.22       695\ninfrastructure_related       0.42      0.07      0.12       328\n             transport       0.63      0.22      0.32       240\n             buildings       0.75      0.39      0.51       267\n           electricity       0.63      0.27      0.38       122\n                 tools       0.00      0.00      0.00        32\n             hospitals       0.25      0.04      0.07        46\n                 shops       0.00      0.00      0.00        22\n           aid_centers       0.50      0.01      0.03        67\n  other_infrastructure       0.22      0.02      0.03       223\n       weather_related       0.80      0.82      0.81      1438\n                floods       0.75      0.61      0.67       411\n                 storm       0.60      0.73      0.66       486\n                  fire       0.65      0.25      0.36        53\n            earthquake       0.86      0.82      0.84       478\n                  cold       0.69      0.34      0.46       117\n         other_weather       0.57      0.17      0.26       276\n         direct_report       0.69      0.56      0.62      1021\n\n             micro avg       0.76      0.65      0.70     16463\n             macro avg       0.59      0.36      0.42     16463\n          weighted avg       0.72      0.65      0.66     16463\n           samples avg       0.63      0.56      0.55     16463\n\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n  'precision', 'predicted', average, warn_for)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n  'precision', 'predicted', average, warn_for)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n  'recall', 'true', average, warn_for)\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0d7ec3dd2e15dbc1b29b1575fd6eaa3779567ec3"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}