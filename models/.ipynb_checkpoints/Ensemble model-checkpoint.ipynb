{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9d2dbdb3-6c74-4f96-9865-2951dfd653ce",
    "_uuid": "bb41ad86b25fecf332927b0c8f55dd710101e33f"
   },
   "source": [
    "# Ensemble of Bidirection LSTM and NB-SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will train Bidirectional LSTM (with Glove Embedding) and NB-SVM separately, and use the average of the predictions as our final prediction. Here is the summary of results:\n",
    "- The LSTM model with Attention turns out to be roughly similar to the one without it\n",
    "- NB-SVM alone does much better than the LSTM model\n",
    "- The ensemble improves NB-SVM's result slightly (roughly 1.5%)\n",
    "\n",
    "This notebook greatly refers to the following Kaggle kernels:\n",
    "- Bidirectional LSTM: https://www.kaggle.com/jhoward/improved-lstm-baseline-glove-dropout\n",
    "- Bidirectional LSTM with Attention: https://www.kaggle.com/suicaokhoailang/lstm-attention-baseline-0-672-lb\n",
    "- Paper: Baselines and Bigrams: Simple, Good Sentiment and Topic Classification, Sida Wang and Christopher D. Manning (https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf)\n",
    "\n",
    "- NB-SVM: AlexSÃ¡nchez's comment on https://www.kaggle.com/jhoward/nb-svm-strong-linear-baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "2f9b7a76-8625-443d-811f-8f49781aef81",
    "_uuid": "598f965bc881cfe6605d92903b758778d400fa8b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "from sklearn.metrics import f1_score, make_scorer, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "7fd4f0d798c5542ee8f707ea68e85008f6041d74",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Reading data file\n",
    "df = pd.read_csv('../input/disaster-data/disaster_data.csv')\n",
    "\n",
    "X = df['message']\n",
    "Y = df.drop(['Unnamed: 0', 'id', 'message', 'genre', 'original', 'child_alone'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "2807a0a5-2220-4af6-92d6-4a7100307de2",
    "_uuid": "d365d5f8d9292bb9bf57d21d6186f8b619cbe8c3"
   },
   "outputs": [],
   "source": [
    "# Basic config\n",
    "EMBEDDING_FILE = './glove6b50d/glove.6B.50d.txt'\n",
    "EMBED_SIZE = 50 # how big is each word vector\n",
    "MAX_FEATURES = 20000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "MAXLEN = 200 # max number of words in a comment to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "183fce594106121f6c3a40c9c0fba818bd1501d3"
   },
   "outputs": [],
   "source": [
    "# Split train-validation set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "list_sentences_train = X_train.values\n",
    "list_sentences_test = X_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "79afc0e9-b5f0-42a2-9257-a72458e91dbb",
    "_uuid": "c292c2830522bfe59d281ecac19f3a9415c07155"
   },
   "outputs": [],
   "source": [
    "# Turn texts into a list of word indexes of equal length\n",
    "tokenizer = Tokenizer(num_words=MAX_FEATURES)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_t = pad_sequences(list_tokenized_train, MAXLEN=MAXLEN)\n",
    "X_te = pad_sequences(list_tokenized_test, MAXLEN=MAXLEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f8c4f6a3-3a19-40b1-ad31-6df2690bec8a",
    "_uuid": "e1cb77629e35c2b5b28288b4d6048a86dda04d78"
   },
   "source": [
    "## Processing Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "7d19392b-7750-4a1b-ac30-ed75b8a62d52",
    "_uuid": "e9e3b4fa7c4658e0f22dd48cb1a289d9deb745fc"
   },
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(MAX_FEATURES, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, EMBED_SIZE))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_FEATURES: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f1aeec65-356e-4430-b99d-bb516ec90b09",
    "_uuid": "237345510bd2e664b5c6983a698d80bac2732bc4"
   },
   "source": [
    "# Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0d4cb718-7f9a-4eab-acda-8f55b4712439",
    "_uuid": "dc51af0bd046e1eccc29111a8e2d77bdf7c60d28"
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(MAXLEN,))\n",
    "x = Embedding(MAX_FEATURES, EMBED_SIZE, weights=[embedding_matrix])(inp)\n",
    "x = Bidirectional(LSTM(100, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(len(Y.columns), activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "333626f1-a838-4fea-af99-0c78f1ef5f5c",
    "_uuid": "c1558c6b2802fc632edc4510c074555a590efbd8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hist = model.fit(X_t, y_train, batch_size=32, epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional LSTM with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "2893e59b260d77f018d3ab0e9b48a5be6e4a341b"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential,Model\n",
    "from keras.layers import CuDNNLSTM, Dense, Bidirectional, Input,Dropout\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "42d0608a02f73da820535d9c8e4316f91faa6057"
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/qqgeogor/keras-lstm-attention-glove840b-lb-0-043\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "47792257b97f3b3246b0f7c2ad07e192f30b98b8"
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(MAXLEN,))\n",
    "x = Embedding(MAX_FEATURES, EMBED_SIZE, weights=[embedding_matrix])(inp)\n",
    "x = Bidirectional(LSTM(128, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "#x = Bidirectional(LSTM(64, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "x = Attention(MAXLEN)(x)\n",
    "# x = GlobalMaxPool1D()(x)\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "x = Dropout(0.2)(x)\n",
    "# x = Dense(64, activation=\"relu\")(x)\n",
    "# x = Dropout(0.2)(x)\n",
    "x = Dense(len(Y.columns), activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "63e0af8e3af9bc541f811fef65f0830cf687e0a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16777 samples, validate on 4195 samples\n",
      "Epoch 1/5\n",
      "16777/16777 [==============================] - 230s 14ms/step - loss: 0.2370 - acc: 0.9180 - val_loss: 0.1868 - val_acc: 0.9353\n",
      "Epoch 2/5\n",
      "16777/16777 [==============================] - 225s 13ms/step - loss: 0.1838 - acc: 0.9365 - val_loss: 0.1672 - val_acc: 0.9432\n",
      "Epoch 3/5\n",
      "16777/16777 [==============================] - 223s 13ms/step - loss: 0.1683 - acc: 0.9420 - val_loss: 0.1596 - val_acc: 0.9458\n",
      "Epoch 4/5\n",
      "16777/16777 [==============================] - 227s 14ms/step - loss: 0.1595 - acc: 0.9446 - val_loss: 0.1560 - val_acc: 0.9471\n",
      "Epoch 5/5\n",
      "16777/16777 [==============================] - 229s 14ms/step - loss: 0.1524 - acc: 0.9466 - val_loss: 0.1573 - val_acc: 0.9468\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_t, y_train, batch_size=32, epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "e1d6f74c1adf42490fa48d89c663cc59ef540677"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5244/5244 [==============================] - 12s 2ms/step\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.89      0.90      0.89      3978\n",
      "               request       0.85      0.57      0.68       895\n",
      "                 offer       0.00      0.00      0.00        26\n",
      "           aid_related       0.79      0.66      0.72      2131\n",
      "          medical_help       0.66      0.05      0.08       422\n",
      "      medical_products       0.89      0.03      0.06       270\n",
      "     search_and_rescue       0.00      0.00      0.00       127\n",
      "              security       0.00      0.00      0.00        88\n",
      "              military       1.00      0.01      0.01       155\n",
      "                 water       0.80      0.18      0.29       339\n",
      "                  food       0.76      0.43      0.55       595\n",
      "               shelter       0.86      0.05      0.10       470\n",
      "              clothing       0.00      0.00      0.00        73\n",
      "                 money       0.00      0.00      0.00       104\n",
      "        missing_people       0.00      0.00      0.00        60\n",
      "              refugees       0.00      0.00      0.00       171\n",
      "                 death       1.00      0.01      0.02       237\n",
      "             other_aid       0.00      0.00      0.00       695\n",
      "infrastructure_related       0.50      0.00      0.01       328\n",
      "             transport       0.00      0.00      0.00       240\n",
      "             buildings       0.67      0.01      0.03       267\n",
      "           electricity       0.00      0.00      0.00       122\n",
      "                 tools       0.00      0.00      0.00        32\n",
      "             hospitals       0.00      0.00      0.00        46\n",
      "                 shops       0.00      0.00      0.00        22\n",
      "           aid_centers       0.00      0.00      0.00        67\n",
      "  other_infrastructure       0.00      0.00      0.00       223\n",
      "       weather_related       0.82      0.78      0.80      1438\n",
      "                floods       0.70      0.36      0.48       411\n",
      "                 storm       0.60      0.57      0.58       486\n",
      "                  fire       0.00      0.00      0.00        53\n",
      "            earthquake       0.88      0.78      0.83       478\n",
      "                  cold       0.00      0.00      0.00       117\n",
      "         other_weather       0.00      0.00      0.00       276\n",
      "         direct_report       0.76      0.49      0.60      1021\n",
      "\n",
      "             micro avg       0.83      0.50      0.63     16463\n",
      "             macro avg       0.38      0.17      0.19     16463\n",
      "          weighted avg       0.69      0.50      0.54     16463\n",
      "           samples avg       0.62      0.43      0.47     16463\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_preds = model.predict([X_te], batch_size=1024, verbose=1)\n",
    "preds = (y_preds > 0.5).astype(np.int)\n",
    "print(classification_report(y_test.values, preds, \n",
    "                            target_names = y_test.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "80ca0031caaddc1c268b3eed409bfc52ad3f4d31"
   },
   "source": [
    "# Build Naive Bayes - SVM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "59ccd7047c7d34de6da82207edd578187d955912"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_is_fitted\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "\n",
    "class NbSvmClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0, dual=False, n_jobs=1):\n",
    "        self.C = C\n",
    "        self.dual = dual\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def predict(self, x):\n",
    "        # Verify that model has been fit\n",
    "        check_is_fitted(self, ['_r', '_clf'])\n",
    "        return self._clf.predict(x.multiply(self._r))\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        # Verify that model has been fit\n",
    "        check_is_fitted(self, ['_r', '_clf'])\n",
    "        return self._clf.predict_proba(x.multiply(self._r))\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        # Check that X and y have correct shape\n",
    "        #y = y.values\n",
    "        x, y = check_X_y(x, y, accept_sparse=True)\n",
    "\n",
    "        def pr(x, y_i, y):\n",
    "            p = x[y==y_i].sum(0)\n",
    "            return (p+1) / ((y==y_i).sum()+1)\n",
    "\n",
    "        self._r = sparse.csr_matrix(np.log(pr(x,1,y) / pr(x,0,y)))\n",
    "        x_nb = x.multiply(self._r)\n",
    "        self._clf = LogisticRegression(C=self.C, dual=self.dual, n_jobs=self.n_jobs).fit(x_nb, y)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "f4f9d9974b043e9685ab6c1d186fa08f50436434"
   },
   "outputs": [],
   "source": [
    "# Text Processing\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import f1_score, make_scorer, classification_report\n",
    "\n",
    "\n",
    "def load_data(database_filepath):\n",
    "    '''\n",
    "    Load data from sqlite database, quick cleaning, and\n",
    "    return the features and labels of the models.\n",
    "\n",
    "    INPUTS:\n",
    "        database_filepath: path to the sqlite database\n",
    "\n",
    "    OUTPUTS:\n",
    "        X, Y: features and labels of the model\n",
    "        Y.columns: categories of the label\n",
    "    '''\n",
    "\n",
    "    engine = create_engine('sqlite:///' + database_filepath)\n",
    "    df = pd.read_sql_table('DisasterResponse', engine)\n",
    "\n",
    "    # 'related' column has values of 0,1,2 which doesn't make sense for binary classification\n",
    "    df['related'] = df['related'].replace(2, 1)\n",
    "\n",
    "    # 'child_alone' column has only value of 0. So our model will always predict 0.\n",
    "    df = df.drop('child_alone', axis=1)\n",
    "\n",
    "    X = df['message']\n",
    "    Y = df.drop(['id', 'message', 'genre', 'original'], axis=1)\n",
    "\n",
    "    return X, Y, Y.columns\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    '''\n",
    "    Preprocess text features by tokenization and lemmatization.\n",
    "\n",
    "    INPUTS:\n",
    "        text: string of text need to be processed\n",
    "\n",
    "    OUTPUTS:\n",
    "        tokens: a list of tokens from the text\n",
    "    '''\n",
    "\n",
    "    # normalize case and remove punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "\n",
    "    # tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # lemmatize andremove stop words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens \\ \n",
    "              if word not in stopwords.words('english')]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def scorer(y_test, y_pred):\n",
    "    '''\n",
    "    Create a evaluation metric for the grid search.\n",
    "    '''\n",
    "\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    weighted_avg = report['weighted avg']\n",
    "    return weighted_avg['f1-score']\n",
    "\n",
    "\n",
    "def build_model(pretrained_model=None):\n",
    "    '''\n",
    "    Build ML pipeline to including text processing and multi-output multi-class classifier\n",
    "    If pretrained_model not None, load pretrained model. \n",
    "    Otherwise output model is a grid search, which takes longer to train.\n",
    "    '''\n",
    "\n",
    "    if pretrained_model != None:\n",
    "        clf = pickle.load(open(pretrained_model, 'rb'))\n",
    "    else:\n",
    "        pipeline = Pipeline([\n",
    "            ('countvec', CountVectorizer(tokenizer=tokenize)),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "            ('clf', MultiOutputClassifier(NbSvmClassifier()))\n",
    "        ])\n",
    "\n",
    "        parameters = {\n",
    "            'clf__estimator__C': [1.0, 5.0, 10.0],\n",
    "            'countvec__ngram_range': [(1, 1), (1, 2)]\n",
    "        }\n",
    "\n",
    "        f1_scorer = make_scorer(scorer)\n",
    "\n",
    "        # optimize model\n",
    "        clf = GridSearchCV(pipeline, parameters, scoring=f1_scorer,\n",
    "                           cv=3, verbose=10)\n",
    "\n",
    "    return clf\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test, Y_test, category_names, pretrained_model=None):\n",
    "    '''\n",
    "    Evaluate the classifier by classification report (sklearn).\n",
    "    If the pretrained_model is None, we trained the grid search, \n",
    "    so best parameters will be reported.\n",
    "    '''\n",
    "    if pretrained_model == None:\n",
    "        print(\"Best parameters: \", model.best_params_)\n",
    "\n",
    "    Y_pred = model.predict(X_test)\n",
    "    report = classification_report(Y_test, Y_pred, target_names=category_names, \n",
    "                                   output_dict=True)\n",
    "\n",
    "    print(\"Validation Results: \")\n",
    "    print(classification_report(Y_test, Y_pred, target_names=category_names))\n",
    "\n",
    "    return (Y_pred, report)\n",
    "\n",
    "\n",
    "def save_model(model, model_filepath):\n",
    "    '''\n",
    "    Save the model to a path specified by model_filepath.\n",
    "    '''\n",
    "    pickle.dump(model, open(model_filepath, 'wb'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "d905a6b75febf4b76fd71fe4ac00005464ea1e57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Training model...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "[CV] clf__estimator__C=1.0, countvec__ngram_range=(1, 1) .............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__C=1.0, countvec__ngram_range=(1, 1), score=0.6174687613469391, total= 1.1min\n",
      "[CV] clf__estimator__C=1.0, countvec__ngram_range=(1, 1) .............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.8min remaining:    0.0s\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__C=1.0, countvec__ngram_range=(1, 1), score=0.6257062108818834, total= 1.1min\n",
      "[CV] clf__estimator__C=1.0, countvec__ngram_range=(1, 1) .............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  3.7min remaining:    0.0s\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__C=1.0, countvec__ngram_range=(1, 1), score=0.6289740579661346, total= 1.1min\n",
      "[CV] clf__estimator__C=1.0, countvec__ngram_range=(1, 2) .............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  5.5min remaining:    0.0s\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__C=1.0, countvec__ngram_range=(1, 2), score=0.6241910613339897, total= 1.3min\n",
      "[CV] clf__estimator__C=1.0, countvec__ngram_range=(1, 2) .............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  7.5min remaining:    0.0s\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__C=1.0, countvec__ngram_range=(1, 2), score=0.6307677048001096, total= 1.3min\n",
      "[CV] clf__estimator__C=1.0, countvec__ngram_range=(1, 2) .............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  9.6min remaining:    0.0s\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__C=1.0, countvec__ngram_range=(1, 2), score=0.6329363273998921, total= 1.3min\n",
      "[CV] clf__estimator__C=5.0, countvec__ngram_range=(1, 1) .............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed: 11.6min remaining:    0.0s\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__C=5.0, countvec__ngram_range=(1, 1), score=0.6284620805508135, total= 1.1min\n",
      "[CV] clf__estimator__C=5.0, countvec__ngram_range=(1, 1) .............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed: 13.4min remaining:    0.0s\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__C=5.0, countvec__ngram_range=(1, 1), score=0.6379854020161618, total= 1.2min\n",
      "[CV] clf__estimator__C=5.0, countvec__ngram_range=(1, 1) .............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed: 15.3min remaining:    0.0s\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__C=5.0, countvec__ngram_range=(1, 1), score=0.6371521421092954, total= 1.1min\n",
      "[CV] clf__estimator__C=5.0, countvec__ngram_range=(1, 2) .............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed: 17.0min remaining:    0.0s\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__C=5.0, countvec__ngram_range=(1, 2), score=0.6414443281483035, total= 1.3min\n",
      "[CV] clf__estimator__C=5.0, countvec__ngram_range=(1, 2) .............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__C=5.0, countvec__ngram_range=(1, 2), score=0.6469481155431975, total= 1.4min\n",
      "[CV] clf__estimator__C=5.0, countvec__ngram_range=(1, 2) .............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__C=5.0, countvec__ngram_range=(1, 2), score=0.6507796391990328, total= 1.4min\n",
      "[CV] clf__estimator__C=10.0, countvec__ngram_range=(1, 1) ............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__C=10.0, countvec__ngram_range=(1, 1), score=0.6270712033821747, total= 1.1min\n",
      "[CV] clf__estimator__C=10.0, countvec__ngram_range=(1, 1) ............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__C=10.0, countvec__ngram_range=(1, 1), score=0.6349160837109199, total= 1.1min\n",
      "[CV] clf__estimator__C=10.0, countvec__ngram_range=(1, 1) ............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__C=10.0, countvec__ngram_range=(1, 1), score=0.6339367125638458, total= 1.1min\n",
      "[CV] clf__estimator__C=10.0, countvec__ngram_range=(1, 2) ............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__C=10.0, countvec__ngram_range=(1, 2), score=0.6409498568105442, total= 1.4min\n",
      "[CV] clf__estimator__C=10.0, countvec__ngram_range=(1, 2) ............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__C=10.0, countvec__ngram_range=(1, 2), score=0.648802592557683, total= 1.4min\n",
      "[CV] clf__estimator__C=10.0, countvec__ngram_range=(1, 2) ............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__C=10.0, countvec__ngram_range=(1, 2), score=0.6508290228506336, total= 1.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  18 out of  18 | elapsed: 35.1min finished\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "Best parameters:  {'clf__estimator__C': 10.0, 'countvec__ngram_range': (1, 2)}\n",
      "Validation Results: \n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.83      0.95      0.88      3978\n",
      "               request       0.80      0.58      0.67       895\n",
      "                 offer       0.00      0.00      0.00        26\n",
      "           aid_related       0.74      0.72      0.73      2131\n",
      "          medical_help       0.61      0.32      0.42       422\n",
      "      medical_products       0.70      0.32      0.44       270\n",
      "     search_and_rescue       0.71      0.17      0.28       127\n",
      "              security       0.40      0.02      0.04        88\n",
      "              military       0.54      0.34      0.42       155\n",
      "                 water       0.73      0.63      0.68       339\n",
      "                  food       0.83      0.72      0.77       595\n",
      "               shelter       0.77      0.59      0.67       470\n",
      "              clothing       0.78      0.44      0.56        73\n",
      "                 money       0.51      0.24      0.33       104\n",
      "        missing_people       1.00      0.15      0.26        60\n",
      "              refugees       0.63      0.30      0.41       171\n",
      "                 death       0.80      0.49      0.60       237\n",
      "             other_aid       0.51      0.14      0.22       695\n",
      "infrastructure_related       0.42      0.06      0.11       328\n",
      "             transport       0.63      0.22      0.32       240\n",
      "             buildings       0.75      0.39      0.51       267\n",
      "           electricity       0.63      0.27      0.38       122\n",
      "                 tools       0.00      0.00      0.00        32\n",
      "             hospitals       0.25      0.04      0.07        46\n",
      "                 shops       0.00      0.00      0.00        22\n",
      "           aid_centers       0.50      0.01      0.03        67\n",
      "  other_infrastructure       0.22      0.02      0.03       223\n",
      "       weather_related       0.82      0.76      0.79      1438\n",
      "                floods       0.86      0.59      0.70       411\n",
      "                 storm       0.70      0.64      0.67       486\n",
      "                  fire       0.65      0.25      0.36        53\n",
      "            earthquake       0.86      0.72      0.78       478\n",
      "                  cold       0.69      0.34      0.46       117\n",
      "         other_weather       0.57      0.17      0.26       276\n",
      "         direct_report       0.70      0.46      0.55      1021\n",
      "\n",
      "             micro avg       0.78      0.62      0.69     16463\n",
      "             macro avg       0.60      0.34      0.41     16463\n",
      "          weighted avg       0.74      0.62      0.65     16463\n",
      "           samples avg       0.63      0.53      0.53     16463\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print('Building model...')\n",
    "model2 = build_model()\n",
    "\n",
    "print('Training model...')\n",
    "model2.fit(X_train, y_train)\n",
    "\n",
    "print('Evaluating model...')\n",
    "preds2, report = evaluate_model(model2, X_test, y_test, y_test.columns)\n",
    "\n",
    "# print('Saving model...\\n    MODEL: {}'.format(model_filepath))\n",
    "# save_model(model, model_filepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4988978e91827bea5e2a7709de04768049feb610"
   },
   "source": [
    "# Simple Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "45123e35ed91a3f8e7a397cdf39b8e93e8de9ca3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.82      0.97      0.89      3978\n",
      "               request       0.78      0.65      0.71       895\n",
      "                 offer       0.00      0.00      0.00        26\n",
      "           aid_related       0.73      0.77      0.75      2131\n",
      "          medical_help       0.60      0.33      0.42       422\n",
      "      medical_products       0.70      0.33      0.45       270\n",
      "     search_and_rescue       0.71      0.17      0.28       127\n",
      "              security       0.40      0.02      0.04        88\n",
      "              military       0.54      0.34      0.42       155\n",
      "                 water       0.70      0.64      0.67       339\n",
      "                  food       0.76      0.75      0.75       595\n",
      "               shelter       0.77      0.60      0.67       470\n",
      "              clothing       0.78      0.44      0.56        73\n",
      "                 money       0.51      0.24      0.33       104\n",
      "        missing_people       1.00      0.15      0.26        60\n",
      "              refugees       0.63      0.30      0.41       171\n",
      "                 death       0.80      0.49      0.61       237\n",
      "             other_aid       0.51      0.14      0.22       695\n",
      "infrastructure_related       0.42      0.07      0.12       328\n",
      "             transport       0.63      0.22      0.32       240\n",
      "             buildings       0.75      0.39      0.51       267\n",
      "           electricity       0.63      0.27      0.38       122\n",
      "                 tools       0.00      0.00      0.00        32\n",
      "             hospitals       0.25      0.04      0.07        46\n",
      "                 shops       0.00      0.00      0.00        22\n",
      "           aid_centers       0.50      0.01      0.03        67\n",
      "  other_infrastructure       0.22      0.02      0.03       223\n",
      "       weather_related       0.80      0.82      0.81      1438\n",
      "                floods       0.75      0.61      0.67       411\n",
      "                 storm       0.60      0.73      0.66       486\n",
      "                  fire       0.65      0.25      0.36        53\n",
      "            earthquake       0.86      0.82      0.84       478\n",
      "                  cold       0.69      0.34      0.46       117\n",
      "         other_weather       0.57      0.17      0.26       276\n",
      "         direct_report       0.69      0.56      0.62      1021\n",
      "\n",
      "             micro avg       0.76      0.65      0.70     16463\n",
      "             macro avg       0.59      0.36      0.42     16463\n",
      "          weighted avg       0.72      0.65      0.66     16463\n",
      "           samples avg       0.63      0.56      0.55     16463\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "ens_preds = (preds + preds2) / 2\n",
    "ens_preds = (ens_preds >= 0.5).astype(np.int)\n",
    "print(classification_report(y_test, ens_preds, target_names=y_test.columns))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
